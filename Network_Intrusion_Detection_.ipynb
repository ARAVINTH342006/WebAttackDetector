{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlHbvGZvrP-e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import joblib\n",
        "import os\n",
        "import gc\n",
        "import logging\n",
        "from typing import List, Dict, Tuple, Optional, Any\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import (classification_report, confusion_matrix,\n",
        "                             roc_auc_score, accuracy_score,\n",
        "                             precision_score, recall_score, f1_score)\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from catboost import CatBoostClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(\"intrusion_detection.log\"),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class NetworkIntrusionDetector:\n",
        "    \"\"\"A professional network intrusion detection system using machine learning\"\"\"\n",
        "\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        \"\"\"\n",
        "        Initialize the intrusion detection system\n",
        "\n",
        "        Args:\n",
        "            config: Configuration dictionary with paths and parameters\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.models = {}\n",
        "        self.results = {}\n",
        "        self.scaler = None\n",
        "\n",
        "        # Create directories for models and plots\n",
        "        os.makedirs(self.config['model_dir'], exist_ok=True)\n",
        "        os.makedirs(self.config['plot_dir'], exist_ok=True)\n",
        "\n",
        "        logger.info(\"Network Intrusion Detector initialized\")\n",
        "\n",
        "    def load_and_preprocess_data(self, file_paths: List[str], dataset_type: str,\n",
        "                                sample_frac: float = 1.0) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Load and preprocess data based on dataset type\n",
        "\n",
        "        Args:\n",
        "            file_paths: List of file paths to load\n",
        "            dataset_type: Type of dataset ('CICIDS' or 'UNSW')\n",
        "            sample_frac: Fraction of data to sample\n",
        "\n",
        "        Returns:\n",
        "            Preprocessed DataFrame\n",
        "        \"\"\"\n",
        "        dfs = []\n",
        "\n",
        "        for file_path in file_paths:\n",
        "            try:\n",
        "                logger.info(f\"Loading data from {file_path}...\")\n",
        "\n",
        "                # Load the data with error handling for different encodings\n",
        "                try:\n",
        "                    df = pd.read_csv(file_path, low_memory=False)\n",
        "                except UnicodeDecodeError:\n",
        "                    df = pd.read_csv(file_path, low_memory=False, encoding='latin-1')\n",
        "\n",
        "                # Clean column names\n",
        "                df.columns = df.columns.str.strip().str.lower()\n",
        "\n",
        "                # Sample data if needed\n",
        "                if sample_frac < 1.0:\n",
        "                    df = df.sample(frac=sample_frac, random_state=self.config['random_state'])\n",
        "                    logger.info(f\"Using {sample_frac*100:.0f}% random sample of data\")\n",
        "\n",
        "                dfs.append(df)\n",
        "                logger.info(f\"Data loaded successfully. Shape: {df.shape}\")\n",
        "\n",
        "            except FileNotFoundError:\n",
        "                logger.error(f\"File not found at {file_path}\")\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error loading {file_path}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if not dfs:\n",
        "            raise ValueError(\"No data files were successfully loaded\")\n",
        "\n",
        "        # Combine all dataframes\n",
        "        combined_df = pd.concat(dfs, ignore_index=True, sort=False)\n",
        "        logger.info(f\"Combined dataset shape: {combined_df.shape}\")\n",
        "\n",
        "        # Preprocess based on dataset type\n",
        "        if dataset_type == \"CICIDS\":\n",
        "            return self._preprocess_cicids_data(combined_df)\n",
        "        elif dataset_type == \"UNSW\":\n",
        "            return self._preprocess_unsw_data(combined_df)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown dataset type: {dataset_type}\")\n",
        "\n",
        "    def _preprocess_cicids_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Preprocessing of CICIDS2017 dataset\"\"\"\n",
        "        logger.info(\"Preprocessing CICIDS2017 data...\")\n",
        "\n",
        "        # Handle missing values and infinities\n",
        "        initial_count = len(df)\n",
        "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "        # Remove columns with too many missing values\n",
        "        missing_threshold = self.config.get('missing_threshold', 0.5)\n",
        "        missing_ratios = df.isnull().sum() / len(df)\n",
        "        cols_to_drop = missing_ratios[missing_ratios > missing_threshold].index\n",
        "\n",
        "        if len(cols_to_drop) > 0:\n",
        "            logger.info(f\"Dropping {len(cols_to_drop)} columns with >{missing_threshold*100}% missing values\")\n",
        "            df.drop(columns=cols_to_drop, inplace=True)\n",
        "\n",
        "        # Remove rows with any remaining missing values\n",
        "        df.dropna(inplace=True)\n",
        "        logger.info(f\"Removed {initial_count - len(df)} rows with issues\")\n",
        "\n",
        "        # Convert labels to binary classification\n",
        "        logger.info(\"Processing labels...\")\n",
        "\n",
        "        # Handle different label formats in CICIDS dataset\n",
        "        if 'label' in df.columns:\n",
        "            df['label'] = df['label'].apply(\n",
        "                lambda x: 0 if 'benign' in str(x).lower() else 1\n",
        "            )\n",
        "        else:\n",
        "            # Try to find label column with different names\n",
        "            label_candidates = ['label', 'class', 'attack', 'result']\n",
        "            for candidate in label_candidates:\n",
        "                if candidate in df.columns:\n",
        "                    df['label'] = df[candidate].apply(\n",
        "                        lambda x: 0 if 'benign' in str(x).lower() or x == 0 else 1\n",
        "                    )\n",
        "                    break\n",
        "            else:\n",
        "                raise ValueError(\"Could not find label column in CICIDS data\")\n",
        "\n",
        "        # Remove any non-binary labels\n",
        "        df = df[df['label'].isin([0, 1])]\n",
        "\n",
        "        # Convert non-numeric columns\n",
        "        logger.info(\"Converting non-numeric features...\")\n",
        "        object_cols = df.select_dtypes(include=['object', 'category']).columns\n",
        "        object_cols = [col for col in object_cols if col != 'label']\n",
        "\n",
        "        for col in object_cols:\n",
        "            try:\n",
        "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Could not convert column {col} to numeric: {e}\")\n",
        "                # If conversion fails, use label encoding\n",
        "                le = LabelEncoder()\n",
        "                df[col] = le.fit_transform(df[col].astype(str))\n",
        "\n",
        "        # Final cleanup\n",
        "        df.dropna(inplace=True)\n",
        "\n",
        "        # Show class distribution\n",
        "        self._log_class_distribution(df)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _preprocess_unsw_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Preprocessing of UNSW-NB15 dataset\"\"\"\n",
        "        logger.info(\"Preprocessing UNSW-NB15 data...\")\n",
        "\n",
        "        # Handle missing values and infinities\n",
        "        initial_count = len(df)\n",
        "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "        # Remove columns with too many missing values\n",
        "        missing_threshold = self.config.get('missing_threshold', 0.5)\n",
        "        missing_ratios = df.isnull().sum() / len(df)\n",
        "        cols_to_drop = missing_ratios[missing_ratios > missing_threshold].index\n",
        "\n",
        "        if len(cols_to_drop) > 0:\n",
        "            logger.info(f\"Dropping {len(cols_to_drop)} columns with >{missing_threshold*100}% missing values\")\n",
        "            df.drop(columns=cols_to_drop, inplace=True)\n",
        "\n",
        "        # Remove rows with any remaining missing values\n",
        "        df.dropna(inplace=True)\n",
        "        logger.info(f\"Removed {initial_count - len(df)} rows with issues\")\n",
        "\n",
        "        # Convert labels to binary classification\n",
        "        logger.info(\"Processing labels...\")\n",
        "\n",
        "        # UNSW-NB15 uses different label column names\n",
        "        label_col = None\n",
        "        for possible_label in ['label', 'attack_cat', 'classification']:\n",
        "            if possible_label in df.columns:\n",
        "                label_col = possible_label\n",
        "                break\n",
        "\n",
        "        if label_col is None:\n",
        "            raise ValueError(\"Could not find label column in UNSW-NB15 data\")\n",
        "\n",
        "        if label_col == 'label':\n",
        "            df['label'] = df['label'].apply(lambda x: 0 if x == 0 else 1)\n",
        "        else:\n",
        "            # For attack_cat or other categorical labels\n",
        "            df['label'] = df[label_col].apply(\n",
        "                lambda x: 0 if str(x).lower() in ['normal', 'benign', '0'] else 1\n",
        "            )\n",
        "\n",
        "        # Remove any non-binary labels\n",
        "        df = df[df['label'].isin([0, 1])]\n",
        "\n",
        "        # Convert non-numeric columns\n",
        "        logger.info(\"Converting non-numeric features...\")\n",
        "        object_cols = df.select_dtypes(include=['object', 'category']).columns\n",
        "        object_cols = [col for col in object_cols if col != 'label']\n",
        "\n",
        "        for col in object_cols:\n",
        "            try:\n",
        "                le = LabelEncoder()\n",
        "                df[col] = le.fit_transform(df[col].astype(str))\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Could not convert column {col} using label encoding: {e}\")\n",
        "                # If conversion fails, drop the column\n",
        "                df.drop(columns=[col], inplace=True)\n",
        "\n",
        "        # Final cleanup\n",
        "        df.dropna(inplace=True)\n",
        "\n",
        "        # Show class distribution\n",
        "        self._log_class_distribution(df)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _log_class_distribution(self, df: pd.DataFrame):\n",
        "        \"\"\"Log class distribution information\"\"\"\n",
        "        class_counts = df['label'].value_counts()\n",
        "        benign_count = class_counts.get(0, 0)\n",
        "        attack_count = class_counts.get(1, 0)\n",
        "\n",
        "        logger.info(\"\\nClass Distribution:\")\n",
        "        logger.info(f\"Benign (0): {benign_count} samples\")\n",
        "        logger.info(f\"Attack (1): {attack_count} samples\")\n",
        "\n",
        "        if attack_count > 0:\n",
        "            imbalance_ratio = benign_count / attack_count\n",
        "            logger.info(f\"Imbalance Ratio: {imbalance_ratio:.2f}:1\")\n",
        "        else:\n",
        "            logger.warning(\"Attack class count is zero, cannot compute imbalance ratio.\")\n",
        "\n",
        "    def _align_features(self, train_df: pd.DataFrame, test_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "        \"\"\"Align features between train and test datasets\"\"\"\n",
        "        # Get common columns\n",
        "        common_cols = list(set(train_df.columns) & set(test_df.columns))\n",
        "\n",
        "        # Ensure label is included\n",
        "        if 'label' not in common_cols:\n",
        "            common_cols.append('label')\n",
        "\n",
        "        # Filter both datasets to only include common columns\n",
        "        train_df = train_df[common_cols]\n",
        "        test_df = test_df[common_cols]\n",
        "\n",
        "        logger.info(f\"Aligned datasets to {len(common_cols)} common features\")\n",
        "\n",
        "        return train_df, test_df\n",
        "\n",
        "    def _remove_low_variance_features(self, df: pd.DataFrame, threshold: float = 0.01) -> pd.DataFrame:\n",
        "        \"\"\"Remove features with low variance\"\"\"\n",
        "        selector = VarianceThreshold(threshold=threshold)\n",
        "        features = df.drop('label', axis=1, errors='ignore')\n",
        "\n",
        "        # Handle case where there are no features left\n",
        "        if features.empty:\n",
        "            return df\n",
        "\n",
        "        selector.fit(features)\n",
        "\n",
        "        # Get features to keep\n",
        "        features_to_keep = features.columns[selector.get_support()]\n",
        "        removed_count = len(features.columns) - len(features_to_keep)\n",
        "\n",
        "        if removed_count > 0:\n",
        "            logger.info(f\"Removing {removed_count} low-variance features\")\n",
        "            return pd.concat([df[features_to_keep], df['label']], axis=1)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _remove_highly_correlated_features(self, df: pd.DataFrame, threshold: float = 0.95) -> pd.DataFrame:\n",
        "        \"\"\"Remove highly correlated features\"\"\"\n",
        "        features = df.drop('label', axis=1, errors='ignore')\n",
        "\n",
        "        # Handle case where there are no features left\n",
        "        if features.empty or len(features.columns) < 2:\n",
        "            return df\n",
        "\n",
        "        # Calculate correlation matrix\n",
        "        try:\n",
        "            corr_matrix = features.corr().abs()\n",
        "        except:\n",
        "            # Fallback to simple correlation calculation\n",
        "            corr_matrix = features.astype(float).corr().abs()\n",
        "\n",
        "        # Select upper triangle of correlation matrix\n",
        "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "        # Find features with correlation greater than threshold\n",
        "        to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
        "\n",
        "        if len(to_drop) > 0:\n",
        "            logger.info(f\"Removing {len(to_drop)} highly correlated features\")\n",
        "            return df.drop(columns=to_drop)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _select_numeric_features(self, df: pd.DataFrame, max_features: int = 30) -> List[str]:\n",
        "        \"\"\"Select only numeric features and limit to top features\"\"\"\n",
        "        # Select only numeric columns\n",
        "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "        # Remove label from features if it's there\n",
        "        if 'label' in numeric_cols:\n",
        "            numeric_cols.remove('label')\n",
        "\n",
        "        logger.info(f\"Found {len(numeric_cols)} numeric features\")\n",
        "\n",
        "        # If too many features, select the most important ones (based on variance)\n",
        "        if len(numeric_cols) > max_features:\n",
        "            logger.info(f\"Selecting top {max_features} features by variance...\")\n",
        "            try:\n",
        "                variances = df[numeric_cols].var().sort_values(ascending=False)\n",
        "                selected_cols = variances.head(max_features).index.tolist()\n",
        "                return selected_cols\n",
        "            except:\n",
        "                # Fallback: just take the first max_features columns\n",
        "                return numeric_cols[:max_features]\n",
        "\n",
        "        return numeric_cols\n",
        "\n",
        "    def _evaluate_model(self, model, X_test: np.ndarray, y_test: np.ndarray,\n",
        "                       model_name: str) -> Dict[str, Any]:\n",
        "        \"\"\"Evaluate model performance\"\"\"\n",
        "        try:\n",
        "            # Generate predictions\n",
        "            y_pred = model.predict(X_test)\n",
        "            y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
        "\n",
        "            # Calculate metrics\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            precision = precision_score(y_test, y_pred, zero_division=0)\n",
        "            recall = recall_score(y_test, y_pred, zero_division=0)\n",
        "            f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "\n",
        "            # Calculate AUC if model supports probability predictions\n",
        "            roc_auc = roc_auc_score(y_test, y_proba) if y_proba is not None else 0.5\n",
        "\n",
        "            # Confusion matrix\n",
        "            cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "            return {\n",
        "                'model_name': model_name,\n",
        "                'accuracy': accuracy,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1_score': f1,\n",
        "                'roc_auc': roc_auc,\n",
        "                'confusion_matrix': cm,\n",
        "                'y_pred': y_pred,\n",
        "                'y_proba': y_proba\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error evaluating model {model_name}: {e}\")\n",
        "            # Return default values in case of error\n",
        "            return {\n",
        "                'model_name': model_name,\n",
        "                'accuracy': 0,\n",
        "                'precision': 0,\n",
        "                'recall': 0,\n",
        "                'f1_score': 0,\n",
        "                'roc_auc': 0.5,\n",
        "                'confusion_matrix': np.zeros((2, 2)),\n",
        "                'y_pred': np.zeros_like(y_test),\n",
        "                'y_proba': None\n",
        "            }\n",
        "\n",
        "    def _plot_comprehensive_comparison(self, results: List[Dict[str, Any]],\n",
        "                                     y_test: np.ndarray, dataset_name: str):\n",
        "        \"\"\"Create comprehensive comparison plots for all models\"\"\"\n",
        "        try:\n",
        "            # 1. Model performance comparison bar chart\n",
        "            metrics_df = pd.DataFrame(results)\n",
        "            metrics_df = metrics_df[['model_name', 'accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']]\n",
        "            metrics_df.set_index('model_name', inplace=True)\n",
        "\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            metrics_df.plot(kind='bar', figsize=(12, 8))\n",
        "            plt.title(f'Model Performance Comparison on {dataset_name}')\n",
        "            plt.ylabel('Score')\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f\"{self.config['plot_dir']}/model_comparison_{dataset_name}.png\",\n",
        "                       dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "            # 2. Confusion matrices for all models\n",
        "            fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "            axes = axes.ravel()\n",
        "\n",
        "            for i, result in enumerate(results):\n",
        "                cm = result['confusion_matrix']\n",
        "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i],\n",
        "                           xticklabels=['Pred Benign', 'Pred Attack'],\n",
        "                           yticklabels=['True Benign', 'True Attack'])\n",
        "                axes[i].set_title(f'{result[\"model_name\"]}\\nAccuracy: {result[\"accuracy\"]:.3f}, F1: {result[\"f1_score\"]:.3f}')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f\"{self.config['plot_dir']}/confusion_matrices_{dataset_name}.png\",\n",
        "                       dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "            # 3. F1-Score comparison\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            f1_scores = [result['f1_score'] for result in results]\n",
        "            model_names = [result['model_name'] for result in results]\n",
        "            colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
        "            plt.bar(model_names, f1_scores, color=colors[:len(model_names)])\n",
        "            plt.title(f'F1-Score Comparison Across Models on {dataset_name}')\n",
        "            plt.ylabel('F1-Score')\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.ylim(0, 1)\n",
        "            for i, v in enumerate(f1_scores):\n",
        "                plt.text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f\"{self.config['plot_dir']}/f1_score_comparison_{dataset_name}.png\",\n",
        "                       dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "            return metrics_df\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error creating plots: {e}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def train_and_evaluate_models(self, X_train: np.ndarray, y_train: np.ndarray,\n",
        "                                 X_test: np.ndarray, y_test: np.ndarray,\n",
        "                                 dataset_name: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Train and evaluate multiple models on the given data\"\"\"\n",
        "        # Feature scaling\n",
        "        logger.info(\"Scaling features...\")\n",
        "        self.scaler = StandardScaler()\n",
        "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
        "        X_test_scaled = self.scaler.transform(X_test)\n",
        "\n",
        "        # Save the scaler\n",
        "        joblib.dump(self.scaler, f\"{self.config['model_dir']}/scaler_{dataset_name}.pkl\")\n",
        "        logger.info(f\"Scaler saved as {self.config['model_dir']}/scaler_{dataset_name}.pkl\")\n",
        "\n",
        "        # Handle class imbalance with SMOTE\n",
        "        logger.info(\"Handling class imbalance with SMOTE...\")\n",
        "        logger.info(f\"Before SMOTE: Class 0: {sum(y_train == 0)}, Class 1: {sum(y_train == 1)}\")\n",
        "\n",
        "        try:\n",
        "            smote = SMOTE(random_state=self.config['random_state'])\n",
        "            X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)\n",
        "            logger.info(f\"After SMOTE: Class 0: {sum(y_train_res == 0)}, Class 1: {sum(y_train_res == 1)}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"SMOTE failed: {e}. Using original data.\")\n",
        "            X_train_res, y_train_res = X_train_scaled, y_train\n",
        "\n",
        "        # Define models with error handling\n",
        "        models = {\n",
        "            \"Logistic Regression\": LogisticRegression(\n",
        "                class_weight='balanced',\n",
        "                random_state=self.config['random_state'],\n",
        "                max_iter=1000,\n",
        "                n_jobs=-1,\n",
        "                solver='saga'\n",
        "            ),\n",
        "            \"Random Forest\": RandomForestClassifier(\n",
        "                n_estimators=100,\n",
        "                max_depth=10,\n",
        "                min_samples_split=5,\n",
        "                min_samples_leaf=2,\n",
        "                class_weight='balanced',\n",
        "                random_state=self.config['random_state'],\n",
        "                n_jobs=-1\n",
        "            ),\n",
        "            \"XGBoost\": XGBClassifier(\n",
        "                n_estimators=200,\n",
        "                learning_rate=0.1,\n",
        "                max_depth=6,\n",
        "                min_child_weight=1,\n",
        "                gamma=0,\n",
        "                subsample=0.8,\n",
        "                colsample_bytree=0.8,\n",
        "                reg_alpha=0,\n",
        "                reg_lambda=1,\n",
        "                use_label_encoder=False,\n",
        "                eval_metric='logloss',\n",
        "                random_state=self.config['random_state'],\n",
        "                n_jobs=-1\n",
        "            ),\n",
        "            \"CatBoost\": CatBoostClassifier(\n",
        "                iterations=200,\n",
        "                learning_rate=0.1,\n",
        "                depth=6,\n",
        "                l2_leaf_reg=3,\n",
        "                random_state=self.config['random_state'],\n",
        "                verbose=False,\n",
        "                thread_count=-1\n",
        "            )\n",
        "        }\n",
        "\n",
        "        # Train and evaluate each model\n",
        "        results = []\n",
        "        for name, model in models.items():\n",
        "            try:\n",
        "                logger.info(f\"\\nTraining {name}...\")\n",
        "                model_start = time.time()\n",
        "\n",
        "                # Fit the model with error handling\n",
        "                try:\n",
        "                    if name == \"XGBoost\":\n",
        "                        model.fit(\n",
        "                            X_train_res, y_train_res,\n",
        "                            eval_set=[(X_test_scaled, y_test)],\n",
        "                            verbose=False\n",
        "                        )\n",
        "                    elif name == \"CatBoost\":\n",
        "                        model.fit(\n",
        "                            X_train_res, y_train_res,\n",
        "                            eval_set=[(X_test_scaled, y_test)],\n",
        "                            verbose=False\n",
        "                        )\n",
        "                    else:\n",
        "                        model.fit(X_train_res, y_train_res)\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error training {name}: {e}\")\n",
        "                    continue\n",
        "\n",
        "                # Save the model\n",
        "                model_path = f\"{self.config['model_dir']}/{name.lower().replace(' ', '_')}_{dataset_name}_model.pkl\"\n",
        "                joblib.dump(model, model_path)\n",
        "                logger.info(f\"{name} model saved as {model_path}\")\n",
        "\n",
        "                # Evaluate the model\n",
        "                model_results = self._evaluate_model(model, X_test_scaled, y_test, name)\n",
        "                model_results['training_time'] = time.time() - model_start\n",
        "                results.append(model_results)\n",
        "\n",
        "                logger.info(f\"{name} training completed in {model_results['training_time']:.2f} seconds\")\n",
        "                logger.info(f\"{name} Accuracy: {model_results['accuracy']:.4f}\")\n",
        "                logger.info(f\"{name} F1-Score: {model_results['f1_score']:.4f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Unexpected error with {name}: {e}\")\n",
        "                continue\n",
        "\n",
        "        return results\n",
        "\n",
        "    def process_dataset(self, train_paths: List[str], test_paths: List[str],\n",
        "                       dataset_name: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Process a complete dataset\"\"\"\n",
        "        logger.info(f\"\\n{'='*60}\")\n",
        "        logger.info(f\"PROCESSING {dataset_name} DATASET\")\n",
        "        logger.info(f\"{'='*60}\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            # Load and preprocess training data\n",
        "            train_df = self.load_and_preprocess_data(train_paths, dataset_name,\n",
        "                                                   self.config['sample_frac'])\n",
        "\n",
        "            # Load and preprocess testing data\n",
        "            test_df = self.load_and_preprocess_data(test_paths, dataset_name,\n",
        "                                                  self.config['sample_frac'])\n",
        "\n",
        "            # Align features between train and test\n",
        "            train_df, test_df = self._align_features(train_df, test_df)\n",
        "\n",
        "            # Enhanced feature selection\n",
        "            logger.info(\"Performing feature selection...\")\n",
        "            train_df = self._remove_low_variance_features(train_df, threshold=0.01)\n",
        "            train_df = self._remove_highly_correlated_features(train_df, threshold=0.95)\n",
        "\n",
        "            # Apply the same feature selection to test data\n",
        "            selected_features = list(train_df.columns)\n",
        "            test_df = test_df[selected_features]\n",
        "\n",
        "            # Select only numeric features\n",
        "            feature_cols = self._select_numeric_features(train_df,\n",
        "                                                       max_features=self.config.get('max_features', 30))\n",
        "\n",
        "            # Prepare features and labels\n",
        "            X_train = train_df[feature_cols]\n",
        "            y_train = train_df['label']\n",
        "\n",
        "            X_test = test_df[feature_cols]\n",
        "            y_test = test_df['label']\n",
        "\n",
        "            logger.info(f\"Selected {len(feature_cols)} features for modeling\")\n",
        "            logger.info(f\"Training set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
        "            logger.info(f\"Testing set: {X_test.shape[0]} samples, {X_test.shape[1]} features\")\n",
        "\n",
        "            # Train and evaluate models\n",
        "            results = self.train_and_evaluate_models(X_train, y_train, X_test, y_test, dataset_name)\n",
        "\n",
        "            # Create comprehensive comparison plots\n",
        "            metrics_df = self._plot_comprehensive_comparison(results, y_test, dataset_name)\n",
        "\n",
        "            if not metrics_df.empty:\n",
        "                # Find best model based on F1-score\n",
        "                best_model_idx = metrics_df['f1_score'].idxmax()\n",
        "                best_model_name = metrics_df.index[best_model_idx]\n",
        "                best_model_score = metrics_df.loc[best_model_name, 'f1_score']\n",
        "\n",
        "                logger.info(f\"\\nBest model for {dataset_name}: {best_model_name} (F1-score: {best_model_score:.4f})\")\n",
        "\n",
        "            logger.info(f\"{dataset_name} processing completed in {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing {dataset_name}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Run the complete intrusion detection pipeline\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        logger.info(\"Starting Network Intrusion Detection Pipeline\")\n",
        "\n",
        "        # Process CICIDS2017 dataset\n",
        "        cicids_results = self.process_dataset(\n",
        "            self.config['cicids_training_paths'],\n",
        "            self.config['cicids_testing_paths'],\n",
        "            \"CICIDS2017\"\n",
        "        )\n",
        "\n",
        "        # Process UNSW-NB15 dataset\n",
        "        unsw_results = self.process_dataset(\n",
        "            self.config['unsw_paths'],\n",
        "            self.config['unsw_paths'],  # Using same paths for train/test for UNSW\n",
        "            \"UNSW_NB15\"\n",
        "        )\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "        logger.info(f\"\\nTotal execution time: {total_time:.2f} seconds\")\n",
        "\n",
        "        # Final message\n",
        "        logger.info(\"\\n\" + \"=\"*60)\n",
        "        logger.info(\"PROCESS COMPLETED!\")\n",
        "        logger.info(\"=\"*60)\n",
        "\n",
        "        return {\n",
        "            'cicids_results': cicids_results,\n",
        "            'unsw_results': unsw_results\n",
        "        }\n",
        "\n",
        "\n",
        "# Configuration\n",
        "config = {\n",
        "    'cicids_training_paths': [\n",
        "        \"/content/drive/MyDrive/Machine learning project/MachineLearningCVE/Monday-WorkingHours.pcap_ISCX.csv\",\n",
        "        \"/content/drive/MyDrive/Machine learning project/MachineLearningCVE/Tuesday-WorkingHours.pcap_ISCX.csv\",\n",
        "        \"/content/drive/MyDrive/Machine learning project/MachineLearningCVE/Wednesday-workingHours.pcap_ISCX.csv\",\n",
        "        \"/content/drive/MyDrive/Machine learning project/MachineLearningCVE/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\",\n",
        "        \"/content/drive/MyDrive/Machine learning project/MachineLearningCVE/Thursday-WorkingHours-Afternoon-Infiltration.pcap_ISCX.csv\"\n",
        "    ],\n",
        "    'cicids_testing_paths': [\n",
        "        \"/content/drive/MyDrive/Machine learning project/MachineLearningCVE/Friday-WorkingHours-Morning.pcap_ISCX.csv\",\n",
        "        \"/content/drive/MyDrive/Machine learning project/MachineLearningCVE/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\"\n",
        "    ],\n",
        "    'unsw_paths': [\n",
        "        \"/content/drive/MyDrive/Machine learning project/MachineLearningCVE/UNSW_NB15_testing-set.csv\"\n",
        "    ],\n",
        "    'sample_frac': 0.2,\n",
        "    'random_state': 42,\n",
        "    'model_dir': \"models\",\n",
        "    'plot_dir': \"performance_plots\",\n",
        "    'max_features': 30\n",
        "}\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Create and run the intrusion detector\n",
        "    detector = NetworkIntrusionDetector(config)\n",
        "    results = detector.run()"
      ]
    }
  ]
}